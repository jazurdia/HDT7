{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo básico con TensorFlow 2.0\n",
    "\n",
    "En este ejercicio vamos a recrear nuestro algoritmo de aprendizaje utilizando TF 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librerías relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de datos\n",
    "\n",
    "Se generan los datos de la misma manera que lo hicimos en el ejercicio anterior.  La única diferencia es que ahora se guardan los datos en un archivo *.npz*.  NPZ es un tipo de archivo propio de NumPy que permite guardar arreglos NumPy.  Se introduce este cambio porque en el ML a menudo:\n",
    "\n",
    "* se reciben datos (csv, base de datos, etc.)\n",
    "* se preprocesan los datos y se dejan en un formato deseado (se verá este tema después)\n",
    "* Tensorflow a menudo no trabaja bien con estos, después de todo trabaja con tensores\n",
    "* se guardan los datos en archivos npz (si es que se está trabajando con Python) para uso posterior \n",
    "\n",
    "No hay nada especial de todo esto.  Solo se guardan arreglos NumPy en un archivo re-utlizable y ya vimos que ya que los  tensores se manejan en format de arreglos de orden N.\n",
    "\n",
    "Se trabajará con dos variables de entrada, x1 y x2.  Estas se generan al azar a partir de una distribución uniforme.\n",
    "\n",
    "Se creará una matriz con estas dos variables.  La matriz X del modelo lineal y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por facilidad, declaramos una variable que indique el tamaño del conjunto \n",
    "#      de datos de entrenamiento.\n",
    "observaciones = 1000\n",
    "\n",
    "x1 = np.random.uniform(low = -10, high = 10, size = (observaciones,1))\n",
    "x2 = np.random.uniform(-10, 10, (observaciones,1)) # nótese que no se requieren las palabras clave\n",
    "\n",
    "X = np.column_stack((x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar las metas a las que debemos apuntar\n",
    "\n",
    "La función a usar: \n",
    "\n",
    "f(x1, x2) = 2 * x1 - 3 * x2 + 5 + <ruido pequeño>\n",
    "\n",
    "El ruido es para hacerlo más realista.\n",
    "\n",
    "En esencia, se puede ver que los pesos serán 2 y -3, y es sesgo es 5\n",
    "\n",
    "Utilizando la metodología de ML, se vé si el algoritmo ha aprendido la función. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
    "\n",
    "y = 2 * x1 - 3 * x2 + 5 + ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora todo ha sido igual.  El siguiente paso sí es nuevo y es que se guarda la información en un archivo *.npz* que se ha denominado \"Datos_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Datos_TF', entradas = X, targets = y)  # nótese que se puede usar cualquier nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolver con TensorFlow\n",
    "\n",
    "<i/>Nota: Esta introducción de TensorFloe es muy básica.  El TF tiene muchas más capacidaded y profundidad que esto.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los datos desde el archivo NPZ.  Por supuesto, esto no era necesario acá\n",
    "datos = np.load('Datos_TF.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejercicio anterior se tuvieron que dar valores iniciales, acá solo se dan:\n",
    "\n",
    "1.  El número de variables de entrada\n",
    "2.  El número de salidas que tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_entrada = 2\n",
    "\n",
    "tamanio_salida = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describir el modelo\n",
    "\n",
    "En otras aplicaciones simplemente se usa un modelo que ya se haya hecho.  Con TensorFlow hay que armarlo\n",
    "\n",
    "Como el ejemplo es muy simple...es lineal...se utiliza el método **\"Sequential\"**\n",
    "\n",
    "Nótese que no se pide cálculo alguno - solo se describe la red\n",
    "\n",
    "Acá se debe listar cada capa \"layer\" de la red neuronal\n",
    "\n",
    "El método **\"Dense\"** realiza una operación matemática **.dot(u, v) + z**.  Esto es exactamente lo que que se necesita: (xw + b)\n",
    "\n",
    "El único parámetro necesario es el tamaño de la salida **tamanio_salida** en el código\n",
    "\n",
    "Hay otros parámetros que se pueden incluir para particularizar el modelo, en este caso, se está tratando de crear una solución que sea tan parecida a la del modelo NumPy anterior por lo que se agregan:\n",
    "\n",
    "* kernel_initializer   -   kernel es un término más generalizado para los pesos\n",
    "* bias_initializer     -   sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "                           \n",
    "                            tf.keras.layers.Dense(tamanio_salida,\n",
    "                                                 kernel_initializer = tf.random_uniform_initializer(minval = -0.1, maxval = 0.1),\n",
    "                                                 bias_initializer = tf.random_uniform_initializer(minval = -0.1, maxval = 0.1)\n",
    "                                                 )\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede definir un optimizador a la medida, en el que se puede especificar la tasa de aprendizaje. De los que hay disponibles, el SGD (Stochastic Gradient Descent) es una generalización de la que se platicó en clase, sin darle nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizador_adhoc = tf.keras.optimizers.SGD(learning_rate = 0.02)\n",
    "optimizador_adhoc = tf.keras.optimizers.SGD(learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la función de pérdida, se utilizará la de **L2-norm**.  Esta también se conoce como **Least sum of squares (Least sum of squares error)**.  El escalamiento se logra obteniendo la media para el # de observaciones.  Esto es justamente lo que hace el **mean squared error**\n",
    "\n",
    "En algún momento, podría ser necesario una función de pérdida hecha a la medida.  Eso es mucho más difícil implementar y no se trabajará en este curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"compile\"** es donde se pueden indicar los optimizadores y la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo.compile(optimizer = optimizador_adhoc, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer = optimizador_adhoc, loss=tf.keras.losses.Huber())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se ajusta el modelo con los datos (o se modelan los datos), indicando las *entradas* y los *targets*...así se denominaron al guardar el archivo.  \n",
    "\n",
    "En vez de usar el término *iteraciones*, se utiliza el término *épocas*.  Si no se especifica el número de épocas este será 1 (una sola época de entrenamiento), así que este número es algo obligatorio.\n",
    "\n",
    "El parámetro **\"verbose\"** se refiere a cuánta información queremos que despliegue durante la ejecución.  Se vale probar diferentes números...se prueban los valores 0, 1 y 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(datos['entradas'], datos['targets'], epochs = 200, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracción de los pesos y sesgos\n",
    "\n",
    "La extracción de el(los) peso(s) y sesgo(s) de un modelo no es un paso esencial para el proceso de ML.  De hecho, en un contexto de aprendizaje profundo, no da mucha información útil.  Sin embargo, este ejemplo simple se armó de tal forma que permite verificar si las respuestas que da el modelo son correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.layers[0].get_weights()    # el cero (0) es porque solo tenemos una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden almacenar los pesos y los sesgos en variables diferentes para facilitar la revisión.\n",
    "\n",
    "OJO!   Pueden haber cientos o miles de estos!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = modelo.layers[0].get_weights()[0]\n",
    "pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sesgos = modelo.layers[0].get_weights()[1]\n",
    "sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer predicciones\n",
    "\n",
    "Una vez más, este no es un paso esencial, sin embargo, generalmente sí se desean hacer predicciones.\n",
    "\n",
    "Se puede pedirle al modelo que prediga nuevos valores.  A veces es útil redondear los valores para que la salida sea más legible.  \n",
    "\n",
    "Generalmente se utiliza este método con DATOS NUEVOS, en vez de usar los datos de entrenamiento originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.predict_on_batch(datos['entradas']).round(1)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se despliegan la metas (valores reales), se pueden comparar visualmente con las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['targets'].round(1)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficar los datos\n",
    "\n",
    "El modelo está ya optimizado, por lo que las salidas se han calculado sobre la última forma, o el último estado, del modelo.\n",
    "\n",
    "Es necesario comprimir o empacar **\"squeeze\"** los arreglos para dejarlos en un formato que es el esperado por la función graficadora.  No cambia nada ya que se dejaron dimensiones de tamaño 1 - solo es un tecnisismo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(modelo.predict_on_batch(datos['entradas'])), \n",
    "         np.squeeze(datos['targets']))\n",
    "plt.xlabel('predicciones')\n",
    "plt.ylabel('metas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Plotly Express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = np.squeeze(modelo.predict_on_batch(datos['entradas'])), \n",
    "                 y =  np.squeeze(datos['targets']))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = \"Comparación predicciones vrs metas\",\n",
    "    xaxis_title = \"Predicciones\",\n",
    "    yaxis_title = \"Metas\",\n",
    "    width = 600,\n",
    "    height = 400,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, lo que se vé debe ser exactamente igual a lo que se vió en el ejercicio pasado!\n",
    "\n",
    "A estas alturas quizás no se le ve la gracia al TensorFlow.  En términos de líneas código es igual al del ejercicio con NumPy para llegar al mismo resultado.  Sin embargo, a medida que se profundice en el tema, se podrá ver que TensorFlow ahorra cientos de líneas de código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9258  \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3653  \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3335  \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4554  \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3874  \n",
      "Iteración 1: Precisión = 0.9654699563980103, Pérdida = 0.3100753128528595\n",
      "Iteración 2: Precisión = 0.34604957699775696, Pérdida = 0.3100753128528595\n",
      "Iteración 3: Precisión = 0.32257166504859924, Pérdida = 0.3100753128528595\n",
      "Iteración 4: Precisión = 0.45998284220695496, Pérdida = 0.3100753128528595\n",
      "Iteración 5: Precisión = 0.3586999177932739, Pérdida = 0.3100753128528595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Número de divisiones para la validación cruzada\n",
    "num_folds = 5\n",
    "\n",
    "# Inicializar el validador cruzado\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Listas para almacenar los resultados de precisión y pérdida\n",
    "precisiones = []\n",
    "perdidas = []\n",
    "\n",
    "# Iterar sobre las divisiones de la validación cruzada\n",
    "for train_index, test_index in kfold.split(datos['entradas']):\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test = datos['entradas'][train_index], datos['entradas'][test_index]\n",
    "    y_train, y_test = datos['targets'][train_index], datos['targets'][test_index]\n",
    "    \n",
    "    # Crear el modelo\n",
    "    modelo = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(tamanio_salida,\n",
    "                              kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                              bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "    ])\n",
    "    \n",
    "    # Crear el optimizador\n",
    "    optimizador_adhoc = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    modelo.compile(optimizer=optimizador_adhoc, loss='mean_squared_error')\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    modelo.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    precision = modelo.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Almacenar precisión y pérdida\n",
    "    precisiones.append(precision)\n",
    "    perdidas.append(resultados)\n",
    "\n",
    "# Reportar resultados de precisión y pérdida para cada iteración de la validación cruzada\n",
    "for i, (precision, perdida) in enumerate(zip(precisiones, perdidas), 1):\n",
    "    print(f\"Iteración {i}: Precisión = {precision}, Pérdida = {perdida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
